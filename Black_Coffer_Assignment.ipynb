{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ccd76ce",
   "metadata": {},
   "source": [
    "# Black Coffer Sentiment Analysis Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5a5479",
   "metadata": {},
   "source": [
    "#### Importing necessary Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc771600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import glob\n",
    "import re\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc00316",
   "metadata": {},
   "source": [
    "#### Reading the Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "42756c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.read_excel('Input.xlsx')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d199a79",
   "metadata": {},
   "source": [
    "#### Having a glance at the top 5 rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "80928367",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL\n",
       "0   123.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "1   321.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "2  2345.0  https://insights.blackcoffer.com/rise-of-e-hea...\n",
       "3  4321.0  https://insights.blackcoffer.com/rise-of-telem...\n",
       "4   432.0  https://insights.blackcoffer.com/rise-of-telem..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0cb702d",
   "metadata": {},
   "source": [
    "#### Finding the shape of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7760d3ee",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(114, 2)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b91184",
   "metadata": {},
   "source": [
    "#### Adding all the headings in a text file named headings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd863550",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "    webpage = requests.get(url).text\n",
    "    soup = BeautifulSoup(webpage, 'lxml')\n",
    "    article_text_list =[]\n",
    "    element1=soup.find_all('h1',class_=\"entry-title\")\n",
    "    element2=soup.find_all('h1',class_=\"tdb-title-text\")\n",
    "    if element1:\n",
    "        article_text_list.append(element1[0].text)\n",
    "    elif element2:\n",
    "        article_text_list.append(element2[0].text)\n",
    "    else:\n",
    "        article_text_list.append(\"Heading not found\")\n",
    "    with open(f\"headings.txt\", 'a') as file:\n",
    "        for item in article_text_list:\n",
    "            file.write(item + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07730f48",
   "metadata": {},
   "source": [
    "#### From the above heading text file we have observed that two URLs with index 24 and 37 doesnt exist, hence we will drop them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e90a7c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop([24,37])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ebd1c",
   "metadata": {},
   "source": [
    "#### Resetting the index after dropping the Invalid URLs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63267d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0b0e1a3",
   "metadata": {},
   "source": [
    "#### Web Scraping the title and content of the URLs using Beautiful soup in a text_files folder, with name of each file based on a URL_ID "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "65dd5f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, row in df.iterrows():\n",
    "    article_text_list =[]\n",
    "    url = row['URL']\n",
    "    url_id = row['URL_ID']\n",
    "    webpage = requests.get(url).text\n",
    "    soup = BeautifulSoup(webpage, 'lxml')\n",
    "    \n",
    "    element1=soup.find_all('h1',class_=\"entry-title\")\n",
    "    element2=soup.find_all('h1',class_=\"tdb-title-text\")\n",
    "    if element1:\n",
    "        article_text_list.append(element1[0].text)\n",
    "    elif element2:\n",
    "        article_text_list.append(element2[0].text)\n",
    "    else:\n",
    "        article_text_list.append(\"Heading not found\")\n",
    "    \n",
    "    div_element1 = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "    div_element2 = soup.find('div', class_='tdb-block-inner td-fix-index') \n",
    "    div_element0 = soup.find_all('div', class_='td_block_wrap tdb_single_content tdi_130 td-pb-border-top td_block_template_1 td-post-content tagdiv-type')\n",
    "\n",
    "    if div_element1:\n",
    "        p_elements = div_element1.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li'], class_=None)\n",
    "\n",
    "        for p_element in p_elements:\n",
    "            article_text_list.append(p_element.get_text())\n",
    "        with open(f\"text_files/{url_id}.txt\", 'a', encoding='utf-8') as file:\n",
    "            for item in article_text_list:\n",
    "                file.write(f\"{item}\\n\")\n",
    "                \n",
    "    elif div_element0:\n",
    "        div_element0=div_element0[0].text\n",
    "        div_element0=soup.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6','li'],class_=None)\n",
    "\n",
    "        for div_elements in div_element0:\n",
    "            article_text_list.append(div_elements.text)\n",
    "        with open(f\"text_files/{url_id}.txt\", 'a', encoding='utf-8') as file:\n",
    "            for item in article_text_list:\n",
    "                file.write(f\"{item}\\n\")\n",
    "    \n",
    "    elif div_element2:\n",
    "        p_elements = div_element2.find_all(['p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'li'], class_=None)\n",
    "\n",
    "        for p_element in p_elements:\n",
    "            article_text_list.append(p_element.get_text())  \n",
    "        with open(f\"text_files/{url_id}.txt\", 'a', encoding='utf-8') as file:\n",
    "            for item in article_text_list:\n",
    "                file.write(f\"{item}\\n\")\n",
    "                \n",
    "    else:\n",
    "        article_text_final.append(\"NOTTTTTTTTTTTTTTTTTTTTT FOUNDDDDDDDDDDDDDDDDDDDDDDDDDDDD\")\n",
    "        with open(f\"text_files/{url_id}.txt\", 'a', encoding='utf-8') as file:\n",
    "            for item in article_text_list:\n",
    "                file.write(f\"{item}\\n\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2603f66a",
   "metadata": {},
   "source": [
    "#### Web scraping a URL seperately which has a different sub div class compared to others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "629620c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "url='https://insights.blackcoffer.com/rise-of-electric-vehicle-and-its-impact-on-livelihood-by-the-year-2040/'\n",
    "url_id=7050.0\n",
    "webpage = requests.get(url).text\n",
    "soup = BeautifulSoup(webpage, 'lxml')\n",
    "article_text_list =[]\n",
    "div_element = soup.find('div', class_='td-post-content tagdiv-type')\n",
    "p_elements = div_element.find_all('p', class_='has-text-align-left')\n",
    "\n",
    "for p_element in p_elements:\n",
    "    article_text_list.append(p_element.get_text())\n",
    "with open(f\"text_files/{url_id}.txt\", 'a', encoding='utf-8') as file:\n",
    "    for item in article_text_list:\n",
    "        file.write(f\"{item}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94532b48",
   "metadata": {},
   "source": [
    "#### Checking for empty text files, to make sure the data is scraped from each URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7556d869",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No empty text files found.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "folder_path = \"text_files\"\n",
    "\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "text_files = [file for file in file_list if file.endswith(\".txt\")]\n",
    "\n",
    "empty_files = []\n",
    "\n",
    "for file_name in text_files:\n",
    "    file_path = os.path.join(folder_path, file_name)\n",
    "    if os.path.getsize(file_path) == 0:\n",
    "        empty_files.append(file_name)\n",
    "\n",
    "if empty_files:\n",
    "    print(\"Empty text files:\")\n",
    "    for file_name in empty_files:\n",
    "        print(file_name)\n",
    "else:\n",
    "    print(\"No empty text files found.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7a37f7f",
   "metadata": {},
   "source": [
    "#### Reading the first file and doing analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b01b5ecf",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"text_files/123.0.txt\"  \n",
    "try:\n",
    "    lines_with_fullstops = []  \n",
    "\n",
    "    with open(file_path, 'r') as file:\n",
    "        for line in file:\n",
    "            line = line.rstrip()          \n",
    "            if not line.endswith('.'):\n",
    "                line += '.'              \n",
    "            lines_with_fullstops.append(line)\n",
    "        \n",
    "except FileNotFoundError:\n",
    "    print(f\"The file '{file_path}' was not found.\")\n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df4bfc7e",
   "metadata": {},
   "source": [
    "#### Calculating the number of files in a folder text_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9273e51e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of files in the folder: 112\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "folder_path = 'text_files'\n",
    "items = os.listdir(folder_path)\n",
    "files = [item for item in items if os.path.isfile(os.path.join(folder_path, item))]\n",
    "number_of_files = len(files)\n",
    "print(f\"Number of files in the folder: {number_of_files}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4c192",
   "metadata": {},
   "source": [
    "#### Extracting the data from each text file and storing it into a variable 'sentences' w.r.t to its URL_ID. Breakdown of paragraphs into individual sentences and storing them as values in a list. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec8aca2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['sentences']=None\n",
    "folder_path = 'text_files'\n",
    "\n",
    "file_list = os.listdir(folder_path)\n",
    "\n",
    "for filename in file_list:\n",
    "    lines_with_fullstops = []\n",
    "    parts = filename.split('.')\n",
    "    url_id_str = '.'.join(parts[:2])\n",
    "    url_id=float(url_id_str)\n",
    "    try:\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            for line in file:\n",
    "                line = line.rstrip()\n",
    "                if not line.endswith('.'):\n",
    "                    line += '.'  \n",
    "                lines_with_fullstops.append(line)\n",
    "         \n",
    "            sentences = []\n",
    "            for paragraph in lines_with_fullstops:\n",
    "                paragraph_sentences = paragraph.split('.')\n",
    "                paragraph_sentences = [sentence.strip() for sentence in paragraph_sentences if sentence.strip()]\n",
    "                sentences.extend(paragraph_sentences)\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")\n",
    "    \n",
    "    matching_indices = df[df['URL_ID'] == url_id].index.tolist()\n",
    "    if matching_indices:\n",
    "        for index in matching_indices:\n",
    "            df.at[index, 'sentences'] = sentences\n",
    "    else:\n",
    "        print(f\"No match found for '{url_id}' in 'URL_ID'\")         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6bc8c791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "112"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df['sentences'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "aa4332c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>sentences</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>[Rise of telemedicine and its Impact on Liveli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>[Rise of e-health and its impact on humans by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>[Rise of e-health and its impact on humans by ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  \\\n",
       "0   123.0  https://insights.blackcoffer.com/rise-of-telem...   \n",
       "1   321.0  https://insights.blackcoffer.com/rise-of-e-hea...   \n",
       "2  2345.0  https://insights.blackcoffer.com/rise-of-e-hea...   \n",
       "\n",
       "                                           sentences  \n",
       "0  [Rise of telemedicine and its Impact on Liveli...  \n",
       "1  [Rise of e-health and its impact on humans by ...  \n",
       "2  [Rise of e-health and its impact on humans by ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f68084f",
   "metadata": {},
   "source": [
    "Extracting stop words from different stop word text files and after lowering into smallcase storing it into a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f76e9bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"StopWords\"\n",
    "stop_words = []\n",
    "\n",
    "def extract_first_word(line):\n",
    "    words = line.split()\n",
    "    if words:\n",
    "        return words[0].strip()  \n",
    "    else:\n",
    "        return None  \n",
    "\n",
    "file_paths = glob.glob(os.path.join(folder_path, \"*.txt\"))\n",
    "\n",
    "for file_path in file_paths:\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            for line in file:\n",
    "                first_word = extract_first_word(line)\n",
    "                if first_word:\n",
    "                    first_word=first_word.lower()\n",
    "                    stop_words.append(first_word)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"The file '{file_path}' was not found.\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing '{file_path}': {str(e)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b232c194",
   "metadata": {},
   "source": [
    "#### Cleaning the data:\n",
    "- Converting into lowercase\n",
    "- Removing stop words and special characters\n",
    "- Storing the cleaned data into a variable named stopwords_removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "feaf16c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def clean_sentence(sentence, stop_words):\n",
    "    sentence = sentence.lower()    \n",
    "    sentence = re.sub(r'[^A-Za-z0-9 ]', ' ', sentence)    \n",
    "    words = word_tokenize(sentence)    \n",
    "    filtered_words = [word for word in words if word not in stop_words]    \n",
    "    cleaned_sentence = ' '.join(filtered_words) \n",
    "    return cleaned_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "786d146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['stopwords_removed']=None\n",
    "for index,i in enumerate(df['sentences']):\n",
    "    cleaned_sentences = []\n",
    "    for sentence in i:\n",
    "        cleaned_sentences.append(clean_sentence(sentence, stop_words))\n",
    "    df.at[index,'stopwords_removed'] = cleaned_sentences\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afd38736",
   "metadata": {},
   "source": [
    "#### Calculating the Total words in a URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b274fa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_word_count']=None\n",
    "for index,i in enumerate(df['stopwords_removed']):\n",
    "    total_word_count = 0\n",
    "    for sentence in i:\n",
    "        words = word_tokenize(sentence)\n",
    "        total_word_count += len(words)\n",
    "    df.at[index,'total_word_count'] = total_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6e91ec35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "cleaned_sentences = []\n",
    "def clean_sentence(sentence, stop_words):\n",
    "    sentence = sentence.lower()    \n",
    "    sentence = re.sub(r'[^A-Za-z0-9 ]', ' ', sentence)    \n",
    "    words = word_tokenize(sentence)    \n",
    "    filtered_words = [word for word in words if word not in stop_words]    \n",
    "    cleaned_sentence = ' '.join(filtered_words) \n",
    "    return cleaned_sentence\n",
    "\n",
    "for sentence in sentences:\n",
    "    cleaned_sentences.append(clean_sentence(sentence, stop_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe99158",
   "metadata": {},
   "source": [
    "#### Extracting Positive words and Negative words from the text files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0e67037",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='MasterDictionary/positive-words.txt'\n",
    "positive_words=[]\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        first_word = extract_first_word(line)\n",
    "        if first_word:\n",
    "            if first_word not in stop_words:\n",
    "                positive_words.append(first_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "92dd6786",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path='MasterDictionary/negative-words.txt'\n",
    "negative_words=[]\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        first_word = extract_first_word(line)\n",
    "        if first_word:\n",
    "            if first_word not in stop_words:\n",
    "                negative_words.append(first_word)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253658fb",
   "metadata": {},
   "source": [
    "#### Calculating positive and negative word count for each URL and storing into variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3df434c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['positive_score']=None\n",
    "for index,i in enumerate(df['stopwords_removed']):\n",
    "    positive_score = 0\n",
    "    for sentence in i:\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            if word.lower() in positive_words:\n",
    "                positive_score += 1\n",
    "    df.at[index,'positive_score'] = positive_score\n",
    "    \n",
    "\n",
    "df['negative_score']=None\n",
    "for index,i in enumerate(df['stopwords_removed']):\n",
    "    negative_score = 0\n",
    "    for sentence in i:\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            if word.lower() in negative_words:\n",
    "                negative_score += 1\n",
    "    df.at[index,'negative_score'] = negative_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba08692",
   "metadata": {},
   "source": [
    "#### Calculating the number of sentences in each article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f8fb4cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['no_of_sentences']=None\n",
    "for index,i in enumerate(df['stopwords_removed']):\n",
    "    no_of_sentences= 0\n",
    "    no_of_sentences = len(i)\n",
    "    df.at[index,'no_of_sentences'] = no_of_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1555617",
   "metadata": {},
   "source": [
    "#### Function to count syllables in a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d7f8d6eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_syllables(word):\n",
    "    if word.endswith(\"es\") or word.endswith(\"ed\"):\n",
    "        return 1     \n",
    "    vowels = \"AEIOUaeiou\"\n",
    "    count = 0\n",
    "    prev_char = ''\n",
    "    \n",
    "    for char in word:\n",
    "        if char in vowels and (prev_char != 'y' or prev_char == word[0]):\n",
    "            count += 1\n",
    "        prev_char = char\n",
    "    return count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be182c3",
   "metadata": {},
   "source": [
    "#### Calculating Complex word count for each URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1fe0e126",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['complex_word_count']=None\n",
    "for index,i in enumerate(df['stopwords_removed']):\n",
    "    complex_word_count = 0\n",
    "    for sentence in i:\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            if count_syllables(word) > 2:\n",
    "                complex_word_count += 1\n",
    "    df.at[index,'complex_word_count'] = complex_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6bcc0c58",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>sentences</th>\n",
       "      <th>stopwords_removed</th>\n",
       "      <th>total_word_count</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>no_of_sentences</th>\n",
       "      <th>complex_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>[Rise of telemedicine and its Impact on Liveli...</td>\n",
       "      <td>[rise telemedicine impact livelihood 2040, tel...</td>\n",
       "      <td>822</td>\n",
       "      <td>79</td>\n",
       "      <td>23</td>\n",
       "      <td>90</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>[Rise of e-health and its impact on humans by ...</td>\n",
       "      <td>[rise health impact humans 2030, rise health e...</td>\n",
       "      <td>274</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>25</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL  \\\n",
       "0   123.0  https://insights.blackcoffer.com/rise-of-telem...   \n",
       "1   321.0  https://insights.blackcoffer.com/rise-of-e-hea...   \n",
       "\n",
       "                                           sentences  \\\n",
       "0  [Rise of telemedicine and its Impact on Liveli...   \n",
       "1  [Rise of e-health and its impact on humans by ...   \n",
       "\n",
       "                                   stopwords_removed total_word_count  \\\n",
       "0  [rise telemedicine impact livelihood 2040, tel...              822   \n",
       "1  [rise health impact humans 2030, rise health e...              274   \n",
       "\n",
       "  positive_score negative_score no_of_sentences complex_word_count  \n",
       "0             79             23              90                490  \n",
       "1             38             13              25                151  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6af1a05",
   "metadata": {},
   "source": [
    "#### Calculating Polarity Score\n",
    "- This is the score that determines if a given text is positive or negative in nature. \n",
    "- It is calculated by using the formula: Polarity Score = (Positive Score – Negative Score)/ ((Positive Score + Negative Score) + 0.000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f1caf1ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Polarity_score'] = (df['positive_score'] - df['negative_score'])/ ((df['positive_score'] + df['negative_score']) + 0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20da8c6f",
   "metadata": {},
   "source": [
    "#### Calculating Subjectivity Score\n",
    "- Subjectivity Score: This is the score that determines if a given text is objective or subjective. \n",
    "- It is calculated by using the formula: Subjectivity Score = (Positive Score + Negative Score)/ ((Total Words after cleaning) + 0.000001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0ae9f74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Subjectivity_score'] = (df['positive_score'] + df['negative_score'])/ ((df['total_word_count']) + 0.000001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a062db3",
   "metadata": {},
   "source": [
    "#### Calculating Average sentence Length\n",
    "- Average Sentence Length = the number of words / the number of sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ea57c8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Average_sentence_length']=df['total_word_count']/df['no_of_sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd00aa15",
   "metadata": {},
   "source": [
    "#### Percentage of Complex words\n",
    "- Percentage of Complex words = the number of complex words / the number of words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b348d236",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Percentage_of_complex_words']=df['complex_word_count']/df['total_word_count']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd9181d1",
   "metadata": {},
   "source": [
    "#### Fog Index = 0.4 * (Average Sentence Length + Percentage of Complex words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "975a8a09",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Fog Index']=0.4 * (df['Average_sentence_length'] + df['Percentage_of_complex_words'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c4eeb4",
   "metadata": {},
   "source": [
    "#### Average Number of Words Per Sentence\n",
    "- The formula for calculating is: Average Number of Words Per Sentence = the total number of words / the total number of sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5503f3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Average_no_of words_per_sentence']=df['total_word_count']/df['no_of_sentences']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93685366",
   "metadata": {},
   "source": [
    "#### Syllable Count Per Word\n",
    "- We count the number of Syllables in each word of the text by counting the vowels present in each word. We also handle some exceptions like words ending with \"es\",\"ed\" by not counting them as a syllable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5b8fee51",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['syllable per word']=None\n",
    "for index,i in enumerate(df['stopwords_removed']):\n",
    "    syllable_dict = {}\n",
    "    for sentence in i:\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            syllable_dict[word] = count_syllables(word)\n",
    "    df.at[index,'syllable per word'] = syllable_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "feae8092",
   "metadata": {},
   "source": [
    "#### Calculating Personal Pronouns\n",
    "- To calculate Personal Pronouns mentioned in the text, we use regex to find the counts of the words - “I,” “we,” “my,” “ours,” and “us”. Special care is taken so that the country name US is not included in the list.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "21f8ec13",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['personal_pronoun_count']=None\n",
    "for index,i in enumerate(df['sentences']):\n",
    "    pronoun_pattern = r'\\b(?:I|we|my|ours|us)\\b'\n",
    "    personal_pronoun_count = 0\n",
    "    for sentence in i:\n",
    "        matches = re.findall(pronoun_pattern, sentence, re.IGNORECASE)\n",
    "        matches = [match for match in matches if match.lower() != \"us\"]\n",
    "        personal_pronoun_count += len(matches)\n",
    "    df.at[index,'personal_pronoun_count'] = personal_pronoun_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f691b1cd",
   "metadata": {},
   "source": [
    "#### Calculating total characters in each URL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "59218f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['total_characters']=None\n",
    "for index,i in enumerate(df['stopwords_removed']):\n",
    "    total_characters = 0\n",
    "    for sentence in i:\n",
    "        words = word_tokenize(sentence)\n",
    "        for word in words:\n",
    "            total_characters += len(word)\n",
    "    df.at[index,'total_characters'] = total_characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55546319",
   "metadata": {},
   "source": [
    "#### Average Word Length is calculated by the formula:\n",
    "- Average Word Length = Sum of the total number of characters in each word / Total number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81caefc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['average_word_length'] = df['total_characters'] / df['total_word_count']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "889bb063",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(['sentences','stopwords_removed','no_of_sentences','total_characters'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "54342438",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>total_word_count</th>\n",
       "      <th>positive_score</th>\n",
       "      <th>negative_score</th>\n",
       "      <th>complex_word_count</th>\n",
       "      <th>Polarity_score</th>\n",
       "      <th>Subjectivity_score</th>\n",
       "      <th>Average_sentence_length</th>\n",
       "      <th>Percentage_of_complex_words</th>\n",
       "      <th>Fog Index</th>\n",
       "      <th>Average_no_of words_per_sentence</th>\n",
       "      <th>syllable per word</th>\n",
       "      <th>personal_pronoun_count</th>\n",
       "      <th>average_word_length</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>822</td>\n",
       "      <td>79</td>\n",
       "      <td>23</td>\n",
       "      <td>490</td>\n",
       "      <td>0.54902</td>\n",
       "      <td>0.124088</td>\n",
       "      <td>9.133333</td>\n",
       "      <td>0.596107</td>\n",
       "      <td>3.891776</td>\n",
       "      <td>9.133333</td>\n",
       "      <td>{'rise': 2, 'telemedicine': 6, 'impact': 2, 'l...</td>\n",
       "      <td>2</td>\n",
       "      <td>7.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>274</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>151</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.186131</td>\n",
       "      <td>10.96</td>\n",
       "      <td>0.551095</td>\n",
       "      <td>4.604438</td>\n",
       "      <td>10.96</td>\n",
       "      <td>{'rise': 2, 'health': 2, 'impact': 2, 'humans'...</td>\n",
       "      <td>3</td>\n",
       "      <td>8.109489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2345.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>548</td>\n",
       "      <td>21</td>\n",
       "      <td>27</td>\n",
       "      <td>241</td>\n",
       "      <td>-0.125</td>\n",
       "      <td>0.087591</td>\n",
       "      <td>5.956522</td>\n",
       "      <td>0.439781</td>\n",
       "      <td>2.558521</td>\n",
       "      <td>5.956522</td>\n",
       "      <td>{'rise': 2, 'health': 2, 'impact': 2, 'humans'...</td>\n",
       "      <td>4</td>\n",
       "      <td>7.282847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>633</td>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>297</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.094787</td>\n",
       "      <td>8.915493</td>\n",
       "      <td>0.469194</td>\n",
       "      <td>3.753875</td>\n",
       "      <td>8.915493</td>\n",
       "      <td>{'rise': 2, 'telemedicine': 6, 'impact': 2, 'l...</td>\n",
       "      <td>7</td>\n",
       "      <td>7.309637</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>432.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>633</td>\n",
       "      <td>34</td>\n",
       "      <td>26</td>\n",
       "      <td>297</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0.094787</td>\n",
       "      <td>8.915493</td>\n",
       "      <td>0.469194</td>\n",
       "      <td>3.753875</td>\n",
       "      <td>8.915493</td>\n",
       "      <td>{'rise': 2, 'telemedicine': 6, 'impact': 2, 'l...</td>\n",
       "      <td>7</td>\n",
       "      <td>7.309637</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL total_word_count  \\\n",
       "0   123.0  https://insights.blackcoffer.com/rise-of-telem...              822   \n",
       "1   321.0  https://insights.blackcoffer.com/rise-of-e-hea...              274   \n",
       "2  2345.0  https://insights.blackcoffer.com/rise-of-e-hea...              548   \n",
       "3  4321.0  https://insights.blackcoffer.com/rise-of-telem...              633   \n",
       "4   432.0  https://insights.blackcoffer.com/rise-of-telem...              633   \n",
       "\n",
       "  positive_score negative_score complex_word_count Polarity_score  \\\n",
       "0             79             23                490        0.54902   \n",
       "1             38             13                151       0.490196   \n",
       "2             21             27                241         -0.125   \n",
       "3             34             26                297       0.133333   \n",
       "4             34             26                297       0.133333   \n",
       "\n",
       "  Subjectivity_score Average_sentence_length Percentage_of_complex_words  \\\n",
       "0           0.124088                9.133333                    0.596107   \n",
       "1           0.186131                   10.96                    0.551095   \n",
       "2           0.087591                5.956522                    0.439781   \n",
       "3           0.094787                8.915493                    0.469194   \n",
       "4           0.094787                8.915493                    0.469194   \n",
       "\n",
       "  Fog Index Average_no_of words_per_sentence  \\\n",
       "0  3.891776                         9.133333   \n",
       "1  4.604438                            10.96   \n",
       "2  2.558521                         5.956522   \n",
       "3  3.753875                         8.915493   \n",
       "4  3.753875                         8.915493   \n",
       "\n",
       "                                   syllable per word personal_pronoun_count  \\\n",
       "0  {'rise': 2, 'telemedicine': 6, 'impact': 2, 'l...                      2   \n",
       "1  {'rise': 2, 'health': 2, 'impact': 2, 'humans'...                      3   \n",
       "2  {'rise': 2, 'health': 2, 'impact': 2, 'humans'...                      4   \n",
       "3  {'rise': 2, 'telemedicine': 6, 'impact': 2, 'l...                      7   \n",
       "4  {'rise': 2, 'telemedicine': 6, 'impact': 2, 'l...                      7   \n",
       "\n",
       "  average_word_length  \n",
       "0            7.833333  \n",
       "1            8.109489  \n",
       "2            7.282847  \n",
       "3            7.309637  \n",
       "4            7.309637  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d185f32b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5a478371",
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.rename(columns={'positive_score' : 'POSITIVE SCORE' , 'negative_score': 'NEGATIVE SCORE', 'total_word_count': 'WORD COUNT', 'complex_word_count': 'COMPLEX WORD COUNT', 'Polarity_score': 'POLARITY SCORE', 'Subjectivity_score': 'SUBJECTIVITY SCORE', 'Average_sentence_length': 'AVG SENTENCE LENGTH', 'Percentage_of_complex_words': 'PERCENTAGE OF COMPLEX WORDS', 'Fog Index': 'FOG INDEX', 'Average_no_of words_per_sentence': 'AVG NUMBER OF WORDS PER SENTENCE', 'syllable per word': 'SYLLABLE PER WORD', 'personal_pronoun_count': 'PERSONAL PRONOUNS', 'average_word_length': 'AVG WORD LENGTH'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "c2994a81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL_ID</th>\n",
       "      <th>URL</th>\n",
       "      <th>WORD COUNT</th>\n",
       "      <th>POSITIVE SCORE</th>\n",
       "      <th>NEGATIVE SCORE</th>\n",
       "      <th>COMPLEX WORD COUNT</th>\n",
       "      <th>POLARITY SCORE</th>\n",
       "      <th>SUBJECTIVITY SCORE</th>\n",
       "      <th>AVG SENTENCE LENGTH</th>\n",
       "      <th>PERCENTAGE OF COMPLEX WORDS</th>\n",
       "      <th>FOG INDEX</th>\n",
       "      <th>AVG NUMBER OF WORDS PER SENTENCE</th>\n",
       "      <th>SYLLABLE PER WORD</th>\n",
       "      <th>PERSONAL PRONOUNS</th>\n",
       "      <th>AVG WORD LENGTH</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>123.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-telem...</td>\n",
       "      <td>822</td>\n",
       "      <td>79</td>\n",
       "      <td>23</td>\n",
       "      <td>490</td>\n",
       "      <td>0.54902</td>\n",
       "      <td>0.124088</td>\n",
       "      <td>9.133333</td>\n",
       "      <td>0.596107</td>\n",
       "      <td>3.891776</td>\n",
       "      <td>9.133333</td>\n",
       "      <td>{'rise': 2, 'telemedicine': 6, 'impact': 2, 'l...</td>\n",
       "      <td>2</td>\n",
       "      <td>7.833333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>321.0</td>\n",
       "      <td>https://insights.blackcoffer.com/rise-of-e-hea...</td>\n",
       "      <td>274</td>\n",
       "      <td>38</td>\n",
       "      <td>13</td>\n",
       "      <td>151</td>\n",
       "      <td>0.490196</td>\n",
       "      <td>0.186131</td>\n",
       "      <td>10.96</td>\n",
       "      <td>0.551095</td>\n",
       "      <td>4.604438</td>\n",
       "      <td>10.96</td>\n",
       "      <td>{'rise': 2, 'health': 2, 'impact': 2, 'humans'...</td>\n",
       "      <td>3</td>\n",
       "      <td>8.109489</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   URL_ID                                                URL WORD COUNT  \\\n",
       "0   123.0  https://insights.blackcoffer.com/rise-of-telem...        822   \n",
       "1   321.0  https://insights.blackcoffer.com/rise-of-e-hea...        274   \n",
       "\n",
       "  POSITIVE SCORE NEGATIVE SCORE COMPLEX WORD COUNT POLARITY SCORE  \\\n",
       "0             79             23                490        0.54902   \n",
       "1             38             13                151       0.490196   \n",
       "\n",
       "  SUBJECTIVITY SCORE AVG SENTENCE LENGTH PERCENTAGE OF COMPLEX WORDS  \\\n",
       "0           0.124088            9.133333                    0.596107   \n",
       "1           0.186131               10.96                    0.551095   \n",
       "\n",
       "  FOG INDEX AVG NUMBER OF WORDS PER SENTENCE  \\\n",
       "0  3.891776                         9.133333   \n",
       "1  4.604438                            10.96   \n",
       "\n",
       "                                   SYLLABLE PER WORD PERSONAL PRONOUNS  \\\n",
       "0  {'rise': 2, 'telemedicine': 6, 'impact': 2, 'l...                 2   \n",
       "1  {'rise': 2, 'health': 2, 'impact': 2, 'humans'...                 3   \n",
       "\n",
       "  AVG WORD LENGTH  \n",
       "0        7.833333  \n",
       "1        8.109489  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e7d727c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_excel('final_output.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873855a2",
   "metadata": {},
   "source": [
    "# Finally, The END !!!!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
